{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d09203eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexey\\venvs\\recsys_project\\Lib\\site-packages\\ray\\thirdparty_files\n",
      "c:\\Users\\Alexey\\Documents\\github\\hse_courses\\2nd_year\\term1\\recsys\\project\n",
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2032.0_x64__qbz5n2kfra8p0\\python311.zip\n",
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2032.0_x64__qbz5n2kfra8p0\\DLLs\n",
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2032.0_x64__qbz5n2kfra8p0\\Lib\n",
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2032.0_x64__qbz5n2kfra8p0\n",
      "c:\\Users\\Alexey\\venvs\\recsys_project\n",
      "\n",
      "c:\\Users\\Alexey\\venvs\\recsys_project\\Lib\\site-packages\n",
      "c:\\Users\\Alexey\\venvs\\recsys_project\\Lib\\site-packages\\win32\n",
      "c:\\Users\\Alexey\\venvs\\recsys_project\\Lib\\site-packages\\win32\\lib\n",
      "c:\\Users\\Alexey\\venvs\\recsys_project\\Lib\\site-packages\\Pythonwin\n",
      "ProtoMF\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "from pathlib import Path\n",
    "protomf_path = Path(\"./ProtoMF/\")\n",
    "sys.path.append(protomf_path.__str__())\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from confs.hyper_params import mf_hyper_params, anchor_hyper_params, user_proto_chose_original_hyper_params, \\\n",
    "    item_proto_chose_original_hyper_params, proto_double_tie_chose_original_hyper_params\n",
    "from experiment_helper import start_hyper, start_multiple_hyper\n",
    "from utilities.consts import SINGLE_SEED\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b02a2629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from ray import tune\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from feature_extraction.feature_extractor_factories import FeatureExtractorFactory\n",
    "from rec_sys_folder.rec_sys import RecSys\n",
    "from utilities.consts import OPTIMIZING_METRIC, MAX_PATIENCE\n",
    "from utilities.eval import Evaluator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse as sp\n",
    "from torch.utils import data\n",
    "from torch.utils.data.dataset import T_co\n",
    "from functools import partial\n",
    "import torch\n",
    "from torch import nn\n",
    "from feature_extraction.feature_extractors import FeatureExtractor\n",
    "import argparse\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from rec_sys_folder.protomf_dataset import get_protorecdataset_dataloader\n",
    "from rec_sys_folder.tester import Tester\n",
    "from rec_sys_folder.trainer import Trainer\n",
    "from utilities.consts import NEG_VAL, OPTIMIZING_METRIC, SEED_LIST, SINGLE_SEED, NUM_SAMPLES, \\\n",
    "    PROJECT_NAME, DATA_PATH, NUM_WORKERS, CPU_PER_TRIAL, GPU_PER_TRIAL, WANDB_API_KEY\n",
    "from ray.air.integrations.wandb import WandbLoggerCallback\n",
    "from utilities.utils import reproducible, generate_id\n",
    "import platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7c4d057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c0ff974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aba4b22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(conf: argparse.Namespace, is_train: bool = True):\n",
    "    if is_train:\n",
    "        train_loader = get_protorecdataset_dataloader(\n",
    "            data_path=conf.data_path,\n",
    "            split_set='train',\n",
    "            n_neg=conf.neg_train,\n",
    "            neg_strategy=conf.train_neg_strategy,\n",
    "            batch_size=conf.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            prefetch_factor=5\n",
    "        )\n",
    "\n",
    "        val_loader = get_protorecdataset_dataloader(\n",
    "            data_path=conf.data_path,\n",
    "            split_set='val',\n",
    "            n_neg=NEG_VAL,\n",
    "            neg_strategy=conf.eval_neg_strategy,\n",
    "            batch_size=conf.val_batch_size,\n",
    "            num_workers=NUM_WORKERS\n",
    "        )\n",
    "        \n",
    "        test_loader = get_protorecdataset_dataloader(\n",
    "            data_path=conf.data_path,\n",
    "            split_set='test',\n",
    "            n_neg=NEG_VAL,\n",
    "            neg_strategy=conf.eval_neg_strategy,\n",
    "            batch_size=conf.val_batch_size,\n",
    "            num_workers=NUM_WORKERS\n",
    "        )\n",
    "\n",
    "        return {'train_loader': train_loader, 'val_loader': val_loader, 'test_loader': test_loader}\n",
    "    else:\n",
    "\n",
    "        test_loader = get_protorecdataset_dataloader(\n",
    "            data_path=conf.data_path,\n",
    "            split_set='test',\n",
    "            n_neg=NEG_VAL,\n",
    "            neg_strategy=conf.eval_neg_strategy,\n",
    "            batch_size=conf.val_batch_size,\n",
    "            num_workers=NUM_WORKERS\n",
    "        )\n",
    "\n",
    "        return {'test_loader': test_loader}\n",
    "\n",
    "\n",
    "def start_training(config):\n",
    "    config = argparse.Namespace(**config)\n",
    "    print(config)\n",
    "\n",
    "    data_loaders_dict = load_data(config)\n",
    "\n",
    "    reproducible(config.seed)\n",
    "\n",
    "    # trainer = Trainer(data_loaders_dict['train_loader'], data_loaders_dict['val_loader'], data_loaders_dict['test_loader'], config)\n",
    "    trainer = Trainer(data_loaders_dict['train_loader'], data_loaders_dict['val_loader'],  config)\n",
    "\n",
    "\n",
    "    trainer.run()\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "def start_testing(config, model_load_path: str):\n",
    "    config = argparse.Namespace(**config)\n",
    "    print(config)\n",
    "\n",
    "    data_loaders_dict = load_data(config, is_train=False)\n",
    "\n",
    "    reproducible(config.seed)\n",
    "\n",
    "    tester = Tester(data_loaders_dict['test_loader'], config, model_load_path)\n",
    "\n",
    "    metric_values = tester.test()\n",
    "    return metric_values\n",
    "\n",
    "\n",
    "def start_hyper(conf: dict, model: str, dataset: str, seed: int = SINGLE_SEED):\n",
    "    print('Starting Hyperparameter Optimization')\n",
    "    print(f'Seed is {seed}')\n",
    "\n",
    "    # Search Algorithm\n",
    "    search_alg = HyperOptSearch(random_state_seed=seed)\n",
    "\n",
    "    if dataset == 'lfm2b-1mon':\n",
    "        scheduler = ASHAScheduler(grace_period=4)\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    # Logger\n",
    "    callback = WandbLoggerCallback(project=PROJECT_NAME, log_config=True, api_key=WANDB_API_KEY,\n",
    "                                   reinit=True, force=True, job_type='train/val', tags=[model, str(seed), dataset])\n",
    "\n",
    "    # Hostname\n",
    "    host_name = platform.uname()\n",
    "\n",
    "    # Dataset\n",
    "    data_path = DATA_PATH\n",
    "    conf['data_path'] = os.path.join(data_path, dataset)\n",
    "\n",
    "    # Seed\n",
    "    conf['seed'] = seed\n",
    "\n",
    "    group_name = f'{model}_{dataset}_{seed}'\n",
    "    tune.register_trainable(group_name, start_training)\n",
    "    analysis = tune.run(\n",
    "        group_name,\n",
    "        config=conf,\n",
    "        name=generate_id(prefix=group_name),\n",
    "        resources_per_trial={'gpu': GPU_PER_TRIAL, 'cpu': CPU_PER_TRIAL},\n",
    "        scheduler=scheduler,\n",
    "        search_alg=search_alg,\n",
    "        num_samples=NUM_SAMPLES,\n",
    "        callbacks=[callback],\n",
    "        metric='_metric/' + OPTIMIZING_METRIC,\n",
    "        mode='max'\n",
    "    )\n",
    "    metric_name = '_metric/' + OPTIMIZING_METRIC\n",
    "    best_trial = analysis.get_best_trial(metric_name, 'max', scope='all')\n",
    "    best_trial_config = best_trial.config\n",
    "    best_trial_checkpoint = os.path.join(analysis.get_best_checkpoint(best_trial, metric_name, 'max'), 'best_model.pth')\n",
    "\n",
    "    wandb.login(key=WANDB_API_KEY)\n",
    "    wandb.init(project=PROJECT_NAME, group='test_results', config=best_trial_config, name=group_name, force=True,\n",
    "               job_type='test', tags=[model, str(seed), dataset])\n",
    "    metric_values = start_testing(best_trial_config, best_trial_checkpoint)\n",
    "    wandb.finish()\n",
    "    return metric_values\n",
    "\n",
    "\n",
    "def start_multiple_hyper(conf: dict, model: str, dataset: str, seed_list: list = SEED_LIST):\n",
    "    print('Starting Multi-Hyperparameter Optimization')\n",
    "    print('seed_list is ', seed_list)\n",
    "    metric_values_list = []\n",
    "    mean_values = dict()\n",
    "\n",
    "    for seed in seed_list:\n",
    "        metric_values_list.append(start_hyper(conf, model, dataset, seed))\n",
    "\n",
    "    for key in metric_values_list[0].keys():\n",
    "        _sum = 0\n",
    "        for metric_values in metric_values_list:\n",
    "            _sum += metric_values[key]\n",
    "        _mean = _sum / len(metric_values_list)\n",
    "\n",
    "        mean_values[key] = _mean\n",
    "\n",
    "    group_name = f'{model}_{dataset}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6ccf8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_param = {\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'n_epochs': 10,\n",
    "    'eval_neg_strategy': 'uniform',\n",
    "    'val_batch_size': 256,\n",
    "    'train_batch_size': 256,\n",
    "    'data_path': protomf_path / \"data/ml\",\n",
    "    'NUM_WORKERS': 1,\n",
    "    'rec_sys_param': {'use_bias': 0},\n",
    "}\n",
    "\n",
    "base_hyper_params = {\n",
    "    **base_param,\n",
    "    'neg_train': 99,\n",
    "    'neg_val': 99,\n",
    "    'train_neg_strategy': 'uniform',#tune.choice(['popular', 'uniform']),\n",
    "    'loss_func_name': 'sampled_softmax', # tune.choice(['bce', 'bpr', 'sampled_softmax']),\n",
    "    'batch_size': np.random.randint(64, 512),\n",
    "    'optim_param': {\n",
    "        'optim': 'adagrad',\n",
    "        'wd': np.random.uniform(low=1e-4, high=1e-2),\n",
    "        'lr': np.random.uniform(low=1e-4, high=1e-1)\n",
    "    },\n",
    "}\n",
    "user_proto_chose_original_hyper_params = {\n",
    "    **base_hyper_params,\n",
    "    'loss_func_aggr': 'mean',\n",
    "    'ft_ext_param': {\n",
    "        \"ft_type\": \"prototypes\",\n",
    "        'embedding_dim': np.random.randint(10, 100), #tune.randint(10, 100),\n",
    "        'user_ft_ext_param': {\n",
    "            \"ft_type\": \"prototypes\",\n",
    "            'sim_proto_weight': np.random.uniform(low=1e-3, high=10), # tune.loguniform(1e-3, 10),\n",
    "            'sim_batch_weight': np.random.uniform(low=1e-3, high=10),\n",
    "            'use_weight_matrix': False,\n",
    "            'n_prototypes': np.random.randint(10, 100), #tune.randint(10, 100),\n",
    "            'cosine_type': 'shifted',\n",
    "            'reg_proto_type': 'max',\n",
    "            'reg_batch_type': 'max',\n",
    "        },\n",
    "        'item_ft_ext_param': {\n",
    "            \"ft_type\": \"embedding\",\n",
    "        }\n",
    "    },\n",
    "}\n",
    "user_proto_chose_original_hyper_params = argparse.Namespace(**user_proto_chose_original_hyper_params)\n",
    "\n",
    "proto_double_tie_chose_original_hyper_params = {\n",
    "    'loss_func_aggr': 'mean',\n",
    "    'ft_ext_param': {\n",
    "        \"ft_type\": \"prototypes_double_tie\",\n",
    "        'embedding_dim': 100, #tune.randint(10, 100),\n",
    "        'item_ft_ext_param': {\n",
    "            \"ft_type\": \"prototypes_double_tie\",\n",
    "            'sim_proto_weight': 1e-3,#tune.loguniform(1e-3, 10),\n",
    "            'sim_batch_weight': 1e-3,#tune.loguniform(1e-3, 10),\n",
    "            'use_weight_matrix': False,\n",
    "            'n_prototypes': 5, #tune.randint(10, 100),\n",
    "            'cosine_type': 'shifted',\n",
    "            'reg_proto_type': 'max',\n",
    "            'reg_batch_type': 'max'\n",
    "        },\n",
    "        'user_ft_ext_param': {\n",
    "            \"ft_type\": \"prototypes_double_tie\",\n",
    "            'sim_proto_weight': 1e-3,#tune.loguniform(1e-3, 10),\n",
    "            'sim_batch_weight': 1e-3, #tune.loguniform(1e-3, 10),\n",
    "            'use_weight_matrix': False,\n",
    "            'n_prototypes': 100, #tune.randint(10, 100),\n",
    "            'cosine_type': 'shifted',\n",
    "            'reg_proto_type': 'max',\n",
    "            'reg_batch_type': 'max'\n",
    "        },\n",
    "    },\n",
    "    \"checkpoint_dir\": 'experiments',\n",
    "    **base_hyper_params,\n",
    "}\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "# proto_double_tie_chose_original_hyper_params = argparse.Namespace(**proto_double_tie_chose_original_hyper_params)\n",
    "# proto_double_tie_chose_original_hyper_params = OmegaConf.create(proto_double_tie_chose_original_hyper_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16f1b92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d16066d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loaders_dict = load_data(proto_double_tie_chose_original_hyper_params)\n",
    "# config = proto_double_tie_chose_original_hyper_params\n",
    "# trainer = Trainer(data_loaders_dict['train_loader'], data_loaders_dict['val_loader'],  config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1459be54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "'user_ft_ext_param'  'n_prototypes': 5,,\n",
    "'item_ft_ext_param' 'n_prototypes': 5,\n",
    " 'embedding_dim': 100,\n",
    "\"\"\"\n",
    "def objective(trial):\n",
    "    prototypes1 = trial.suggest_int('prototypes1', 20, 100, 20)\n",
    "    prototypes2 = trial.suggest_int('prototypes2', 20, 100, 20)\n",
    "\n",
    "    embeddings_dim = trial.suggest_int(\"embedding\", 50, 400, 50)\n",
    "    \n",
    "    proto_double_tie_chose_original_hyper_params['ft_ext_param']['user_ft_ext_param']['n_prototypes'] = prototypes1\n",
    "    proto_double_tie_chose_original_hyper_params['ft_ext_param']['item_ft_ext_param']['n_prototypes'] = prototypes2\n",
    "    proto_double_tie_chose_original_hyper_params['embedding_dim'] = embeddings_dim\n",
    "    \n",
    "    \n",
    "    config = argparse.Namespace(**proto_double_tie_chose_original_hyper_params)\n",
    "    data_loaders_dict = load_data(config)\n",
    "\n",
    "    trainer = Trainer(data_loaders_dict['train_loader'], data_loaders_dict['val_loader'],  config)\n",
    "    \n",
    "    return trainer.run(trial)\n",
    "\n",
    "# study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2a21805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RecSys(nn.Module):\n",
    "\n",
    "#     def __init__(self, n_users: int, n_items: int, rec_sys_param, user_feature_extractor: FeatureExtractor,\n",
    "#                  item_feature_extractor: FeatureExtractor, loss_func_name: str, loss_func_aggr: str = 'mean'):\n",
    "#         \"\"\"\n",
    "#         General Recommender System\n",
    "#         It generates the user/item vectors (given the feature extractors) and computes the similarity by the dot product.\n",
    "#         :param n_users: number of users in the system\n",
    "#         :param n_items: number of items in the system\n",
    "#         :param rec_sys_param: parameters of the Recommender System module\n",
    "#         :param user_feature_extractor: feature_extractor.FeatureExtractor module that generates user embeddings.\n",
    "#         :param item_feature_extractor: feature_extractor.FeatureExtractor module that generates item embeddings.\n",
    "#         :param loss_func_name: name of the loss function to use for the network.\n",
    "#         :param loss_func_aggr: type of aggregation for the loss function, either 'mean' or 'sum'.\n",
    "#         \"\"\"\n",
    "\n",
    "#         assert loss_func_aggr in ['mean', 'sum'], f'Loss function aggregators <{loss_func_aggr}> not implemented...yet'\n",
    "\n",
    "#         super().__init__()\n",
    "#         self.n_users = n_users\n",
    "#         self.n_items = n_items\n",
    "#         self.rec_sys_param = rec_sys_param\n",
    "#         self.user_feature_extractor = user_feature_extractor\n",
    "#         self.item_feature_extractor = item_feature_extractor\n",
    "#         self.loss_func_name = loss_func_name\n",
    "#         self.loss_func_aggr = loss_func_aggr\n",
    "\n",
    "#         self.use_bias = self.rec_sys_param[\"use_bias\"] > 0 if 'use_bias' in self.rec_sys_param else True\n",
    "\n",
    "#         if self.use_bias:\n",
    "#             self.user_bias = nn.Embedding(self.n_users, 1)\n",
    "#             self.item_bias = nn.Embedding(self.n_items, 1)\n",
    "#             self.global_bias = nn.Parameter(torch.zeros(1), requires_grad=True)\n",
    "\n",
    "#         if self.loss_func_name == 'bce':\n",
    "#             self.rec_loss = partial(bce_loss, aggregator=self.loss_func_aggr)\n",
    "#         elif self.loss_func_name == 'bpr':\n",
    "#             self.rec_loss = partial(bpr_loss, aggregator=self.loss_func_aggr)\n",
    "#         elif self.loss_func_name == 'sampled_softmax':\n",
    "#             self.rec_loss = partial(sampled_softmax_loss, aggregator=self.loss_func_aggr)\n",
    "#         else:\n",
    "#             raise ValueError(f'Recommender System Loss function <{self.rec_loss}> Not Implemented... Yet')\n",
    "\n",
    "#         self.initialized = False\n",
    "\n",
    "#         print(f'Built RecSys module \\n'\n",
    "#               f'- n_users: {self.n_users} \\n'\n",
    "#               f'- n_items: {self.n_items} \\n'\n",
    "#               f'- user_feature_extractor: {self.user_feature_extractor.name} \\n'\n",
    "#               f'- item_feature_extractor: {self.item_feature_extractor.name} \\n'\n",
    "#               f'- loss_func_name: {self.loss_func_name} \\n'\n",
    "#               f'- use_bias: {self.use_bias} \\n')\n",
    "\n",
    "#     def init_parameters(self):\n",
    "#         \"\"\"\n",
    "#         Method for initializing the Recommender System Processor\n",
    "#         \"\"\"\n",
    "#         if self.use_bias:\n",
    "#             torch.nn.init.constant_(self.user_bias.weight, 0.)\n",
    "#             torch.nn.init.constant_(self.item_bias.weight, 0.)\n",
    "\n",
    "#         self.user_feature_extractor.init_parameters()\n",
    "#         self.item_feature_extractor.init_parameters()\n",
    "\n",
    "#         self.initialized = True\n",
    "\n",
    "#     def loss_func(self, logits, labels):\n",
    "#         \"\"\"\n",
    "#         Loss function of the Recommender System module. It takes into account eventual feature_extractor loss terms.\n",
    "#         NB. Any feature_extractor loss is pre-weighted.\n",
    "#         :param logits: output of the system.\n",
    "#         :param labels: binary labels\n",
    "#         :return: aggregated loss\n",
    "#         \"\"\"\n",
    "\n",
    "#         rec_loss = self.rec_loss(logits, labels)\n",
    "#         item_feat_ext_loss = self.item_feature_extractor.get_and_reset_loss()\n",
    "#         user_feat_ext_loss = self.user_feature_extractor.get_and_reset_loss()\n",
    "#         return rec_loss + item_feat_ext_loss + user_feat_ext_loss\n",
    "\n",
    "#     def forward(self, u_idxs, i_idxs):\n",
    "#         \"\"\"\n",
    "#         Performs the forward pass considering user indexes and the item indexes. Negative Sampling is done automatically\n",
    "#         by the dataloader\n",
    "#         :param u_idxs: User indexes. Shape is (batch_size,)\n",
    "#         :param i_idxs: Item indexes. Shape is (batch_size, n_neg + 1)\n",
    "\n",
    "#         :return: A matrix of logits values. Shape is (batch_size, 1 + n_neg). First column is always associated\n",
    "#                 to the positive track.\n",
    "#         \"\"\"\n",
    "#         assert self.initialized, 'Model initialization has not been called! Please call .init_parameters() ' \\\n",
    "#                                  'before using the model'\n",
    "\n",
    "#         # --- User pass ---\n",
    "#         u_embed = self.user_feature_extractor(u_idxs)\n",
    "#         if self.use_bias:\n",
    "#             u_bias = self.user_bias(u_idxs)\n",
    "\n",
    "#         # --- Item pass ---\n",
    "#         if self.use_bias:\n",
    "#             i_bias = self.item_bias(i_idxs).squeeze()\n",
    "\n",
    "#         i_embed = self.item_feature_extractor(i_idxs)\n",
    "\n",
    "#         # --- Dot Product ---\n",
    "#         dots = torch.sum(u_embed.unsqueeze(1) * i_embed, dim=-1)  # [batch_size, n_neg_p_1]\n",
    "\n",
    "#         if self.use_bias:\n",
    "#             # Optional bias\n",
    "#             dots = dots + u_bias + i_bias + self.global_bias\n",
    "\n",
    "#         return dots\n",
    "\n",
    "\n",
    "# def bce_loss(logits, labels, aggregator='mean'):\n",
    "#     \"\"\"\n",
    "#     It computes the binary cross entropy loss with negative sampling, expressed by the formula:\n",
    "#                                     -∑_j log(x_ui) + log(1 - x_uj)\n",
    "#     where x_ui and x_uj are the prediction for user u on item i and j, respectively. Item i positive instance while\n",
    "#     Item j is a negative instance. The Sum is carried out across the different negative instances. In other words\n",
    "#     the positive item is weighted as many as negative items are considered.\n",
    "\n",
    "#     :param logits: Logits values from the network. The first column always contain the values of positive instances.\n",
    "#             Shape is (batch_size, 1 + n_neg).\n",
    "#     :param labels: 1-0 Labels. The first column contains 1s while all the others 0s.\n",
    "#     :param aggregator: function to use to aggregate the loss terms. Default to mean\n",
    "\n",
    "#     :return: The binary cross entropy as computed above\n",
    "#     \"\"\"\n",
    "#     weights = torch.ones_like(logits)\n",
    "#     weights[:, 0] = logits.shape[1] - 1\n",
    "\n",
    "#     loss = nn.BCEWithLogitsLoss(weights.flatten(), reduction=aggregator)(logits.flatten(), labels.flatten())\n",
    "\n",
    "#     return loss\n",
    "\n",
    "\n",
    "# def bpr_loss(logits, labels, aggregator='mean'):\n",
    "#     \"\"\"\n",
    "#     It computes the Bayesian Personalized Ranking loss (https://arxiv.org/pdf/1205.2618.pdf).\n",
    "\n",
    "#     :param logits: Logits values from the network. The first column always contain the values of positive instances.\n",
    "#             Shape is (batch_size, 1 + n_neg).\n",
    "#     :param labels: 1-0 Labels. The first column contains 1s while all the others 0s.\n",
    "#     :param aggregator: function to use to aggregate the loss terms. Default to mean\n",
    "\n",
    "#     :return: The bayesian personalized ranking loss\n",
    "#     \"\"\"\n",
    "#     pos_logits = logits[:, 0].unsqueeze(1)  # [batch_size,1]\n",
    "#     neg_logits = logits[:, 1:]  # [batch_size,n_neg]\n",
    "\n",
    "#     labels = labels[:, 0]  # I guess this is just to avoid problems with the device\n",
    "#     labels = torch.repeat_interleave(labels, neg_logits.shape[1])\n",
    "\n",
    "#     diff_logits = pos_logits - neg_logits\n",
    "\n",
    "#     loss = nn.BCEWithLogitsLoss(reduction=aggregator)(diff_logits.flatten(), labels.flatten())\n",
    "\n",
    "#     return loss\n",
    "\n",
    "\n",
    "# def sampled_softmax_loss(logits, labels, aggregator='sum'):\n",
    "#     \"\"\"\n",
    "#     It computes the (Sampled) Softmax Loss (a.k.a. sampled cross entropy) expressed by the formula:\n",
    "#                         -x_ui +  log( ∑_j e^{x_uj})\n",
    "#     where x_ui and x_uj are the prediction for user u on item i and j, respectively. Item i positive instance while j\n",
    "#     goes over all the sampled items (negatives + the positive).\n",
    "#     :param logits: Logits values from the network. The first column always contain the values of positive instances.\n",
    "#             Shape is (batch_size, 1 + n_neg).\n",
    "#     :param labels: 1-0 Labels. The first column contains 1s while all the others 0s.\n",
    "#     :param aggregator: function to use to aggregate the loss terms. Default to sum\n",
    "#     :return:\n",
    "#     \"\"\"\n",
    "\n",
    "#     pos_logits_sum = - logits[:, 0]\n",
    "#     log_sum_exp_sum = torch.logsumexp(logits, dim=-1)\n",
    "\n",
    "#     sampled_loss = pos_logits_sum + log_sum_exp_sum\n",
    "\n",
    "#     if aggregator == 'sum':\n",
    "#         return sampled_loss.sum()\n",
    "#     elif aggregator == 'mean':\n",
    "#         return sampled_loss.mean()\n",
    "#     else:\n",
    "#         raise ValueError('Loss aggregator not defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96812ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = Path(\"./ProtoMF/best_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55f30b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Trainer:\n",
    "\n",
    "#     def __init__(self, train_loader: data.DataLoader, val_loader: data.DataLoader, conf):\n",
    "#         \"\"\"\n",
    "#         Train and Evaluate the model.\n",
    "#         :param train_loader: Training DataLoader (check music4all_data.Music4AllDataset for more info)\n",
    "#         :param val_loader: Validation DataLoader (check music4all_data.Music4AllDataset for more info)\n",
    "#         :param conf: Experiment configuration parameters\n",
    "#         \"\"\"\n",
    "\n",
    "#         self.train_loader = train_loader\n",
    "#         self.val_loader = val_loader\n",
    "\n",
    "#         self.rec_sys_param = conf.rec_sys_param\n",
    "#         self.ft_ext_param = conf.ft_ext_param\n",
    "#         self.optim_param = conf.optim_param\n",
    "\n",
    "#         self.n_epochs = conf.n_epochs\n",
    "#         self.loss_func_name = conf.loss_func_name\n",
    "#         self.loss_func_aggr = conf.loss_func_aggr if 'loss_func_aggr' in conf else 'mean'\n",
    "\n",
    "#         self.device = conf.device\n",
    "\n",
    "#         self.optimizing_metric = OPTIMIZING_METRIC\n",
    "#         self.max_patience = MAX_PATIENCE\n",
    "\n",
    "#         self.model = self._build_model()\n",
    "#         self.optimizer = self._build_optimizer()\n",
    "\n",
    "#         print(f'Built Trainer module \\n'\n",
    "#               f'- n_epochs: {self.n_epochs} \\n'\n",
    "#               f'- loss_func_name: {self.loss_func_name} \\n'\n",
    "#               f'- loss_func_aggr: {self.loss_func_aggr} \\n'\n",
    "#               f'- device: {self.device} \\n'\n",
    "#               f'- optimizing_metric: {self.optimizing_metric} \\n')\n",
    "\n",
    "#     def _build_model(self):\n",
    "#         # Step 1 --- Building User and Item Feature Extractors\n",
    "#         n_users = self.train_loader.dataset.n_users\n",
    "#         n_items = self.train_loader.dataset.n_items\n",
    "#         user_feature_extractor, item_feature_extractor = \\\n",
    "#             FeatureExtractorFactory.create_models(self.ft_ext_param, n_users, n_items)\n",
    "#         # Step 2 --- Building RecSys Module\n",
    "#         rec_sys = RecSys(n_users, n_items, self.rec_sys_param, user_feature_extractor, item_feature_extractor,\n",
    "#                          self.loss_func_name, self.loss_func_aggr)\n",
    "\n",
    "#         rec_sys.init_parameters()\n",
    "#         rec_sys = nn.DataParallel(rec_sys)\n",
    "#         rec_sys = rec_sys.to(self.device)\n",
    "\n",
    "#         return rec_sys\n",
    "\n",
    "#     def _build_optimizer(self):\n",
    "#         self.lr = self.optim_param['lr'] if 'lr' in self.optim_param else 1e-3\n",
    "#         self.wd = self.optim_param['wd'] if 'wd' in self.optim_param else 1e-4\n",
    "\n",
    "#         optim_name = self.optim_param['optim']\n",
    "#         if optim_name == 'adam':\n",
    "#             optim = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "#         elif optim_name == 'adagrad':\n",
    "#             optim = torch.optim.Adagrad(self.model.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "#         else:\n",
    "#             raise ValueError('Optimizer not yet included')\n",
    "\n",
    "#         print(f'Built Optimizer  \\n'\n",
    "#               f'- name: {optim_name} \\n'\n",
    "#               f'- lr: {self.lr} \\n'\n",
    "#               f'- wd: {self.wd} \\n')\n",
    "\n",
    "#         return optim\n",
    "\n",
    "#     def run(self):\n",
    "#         \"\"\"\n",
    "#         Runs the Training procedure\n",
    "#         \"\"\"\n",
    "#         metrics_values = self.val()\n",
    "#         best_value = metrics_values[self.optimizing_metric]\n",
    "# #         tune.report(metrics_values)\n",
    "#         print('Init - Avg Val Value {:.3f} \\n'.format(best_value))\n",
    "\n",
    "#         patience = 0\n",
    "#         for epoch in range(self.n_epochs):\n",
    "\n",
    "#             if patience == self.max_patience:\n",
    "#                 print('Max Patience reached, stopping.')\n",
    "#                 break\n",
    "\n",
    "#             self.model.train()\n",
    "\n",
    "#             epoch_train_loss = 0\n",
    "\n",
    "#             for u_idxs, i_idxs, labels in self.train_loader:\n",
    "#                 # print(u_idxs.shape)\n",
    "#                 # print(i_idxs.shape)\n",
    "#                 # print(labels.shape)\n",
    "\n",
    "#                 u_idxs = u_idxs.to(self.device)\n",
    "#                 i_idxs = i_idxs.to(self.device)\n",
    "#                 labels = labels.to(self.device)\n",
    "\n",
    "#                 out = self.model(u_idxs, i_idxs)\n",
    "\n",
    "#                 loss = self.model.module.loss_func(out, labels)\n",
    "\n",
    "#                 epoch_train_loss += loss.item()\n",
    "\n",
    "#                 loss.backward()\n",
    "#                 self.optimizer.step()\n",
    "#                 self.optimizer.zero_grad()\n",
    "#                 if int(u_idxs[0]) % 1000 == 0:\n",
    "#                     print(str(int(u_idxs[0])) + '_users_past')\n",
    "#             epoch_train_loss /= len(self.train_loader)\n",
    "#             print(\"Epoch {} - Epoch Avg Train Loss {:.3f} \\n\".format(epoch, epoch_train_loss))\n",
    "\n",
    "#             metrics_values = self.val()\n",
    "#             curr_value = metrics_values[self.optimizing_metric]\n",
    "#             print('Epoch {} - Avg Val Value {:.3f} \\n'.format(epoch, curr_value))\n",
    "#             # tune.report({**metrics_values, 'epoch_train_loss': epoch_train_loss})\n",
    "\n",
    "#             if curr_value > best_value:\n",
    "#                 best_value = curr_value\n",
    "#                 print('Epoch {} - New best model found (val value {:.3f}) \\n'.format(epoch, curr_value))\n",
    "#                 torch.save(self.model.module.state_dict(), os.path.join(checkpoint_dir, 'best_model.pth'))\n",
    "#                 patience = 0\n",
    "#             else:\n",
    "#                 patience += 1\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def val(self):\n",
    "#         \"\"\"\n",
    "#         Runs the evaluation procedure.\n",
    "#         :return: A scalar float value, output of the validation (e.g. NDCG@10).\n",
    "#         \"\"\"\n",
    "#         self.model.eval()\n",
    "#         print('Validation started')\n",
    "#         val_loss = 0\n",
    "#         eval = Evaluator(self.val_loader.dataset.n_users)\n",
    "\n",
    "#         for u_idxs, i_idxs, labels in self.val_loader:\n",
    "#             u_idxs = u_idxs.to(self.device)\n",
    "#             i_idxs = i_idxs.to(self.device)\n",
    "#             labels = labels.to(self.device)\n",
    "\n",
    "#             out = self.model(u_idxs, i_idxs)\n",
    "\n",
    "#             val_loss += self.model.module.loss_func(out, labels).item()\n",
    "\n",
    "#             out = nn.Sigmoid()(out)\n",
    "#             out = out.to('cpu')\n",
    "\n",
    "#             eval.eval_batch(out)\n",
    "#             if int(u_idxs[0]) % 1000 == 0:\n",
    "#                 print(str(int(u_idxs[0])) + '_users_past')\n",
    "#         val_loss /= len(self.val_loader)\n",
    "#         metrics_values = {**eval.get_results(), 'val_loss': val_loss}\n",
    "\n",
    "#         return metrics_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff1b2309",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtoRecDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class to be used in ProtoRec. To use this class for any dataset, please refer to the splitter functions\n",
    "    (e.g. movielens_splitter.py)\n",
    "\n",
    "    This class implements some basic functionalities about negative sampling. The negative sampling for a specific user\n",
    "    is influenced by the split_set:\n",
    "        - split_set = train: The other training items are excluded from the sampling.\n",
    "        - split_set = val: The other validation items and training items are excluded from the sampling.\n",
    "        - split_set = test: The other test items and training items are excluded from the sampling.\n",
    "\n",
    "    About the data management and access:\n",
    "    To perform a fast iteration and sampling over the dataset, we use two sparse matrices (COO and CSR). The COO\n",
    "    is used for iteration over the training data while the CSR for fast negative sampling. We always load the train\n",
    "    CSR since it is used to exclude the training data from the negative sampling also for Validation and Testing.\n",
    "    NB. Depending on the split_set, the matrices may have different data. Train COO and Train CSR have always the\n",
    "    same data. However, Val CSR has Val + Train data (same applies for test). This is due to the negative sampling\n",
    "    in the csr matrix, for which we also exclude items from training (see below).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str, split_set: str, n_neg: int, neg_strategy: str = 'uniform'):\n",
    "        \"\"\"\n",
    "        :param data_path: path to the directory with the listening_history_*, item_ids, and user_ids files.\n",
    "        :param split_set: Value in [train, val, test].\n",
    "        :param n_neg: Number of negative samples.\n",
    "        :param neg_strategy: Strategy to select the negative samples.\n",
    "        \"\"\"\n",
    "        assert split_set in ['train', 'val', 'test'], f'<{split_set}> is not a valid value for split set!'\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.split_set = split_set\n",
    "        self.n_neg = n_neg\n",
    "        self.neg_strategy = neg_strategy\n",
    "\n",
    "        self.n_users = None\n",
    "        self.n_items = None\n",
    "\n",
    "        self.item_ids = None\n",
    "\n",
    "        self.coo_matrix = None\n",
    "        self.csr_matrix = None\n",
    "\n",
    "        self.pop_distribution = None\n",
    "\n",
    "        self.load_data()\n",
    "\n",
    "        print(f'Built ProtoRecDataset module \\n'\n",
    "              f'- data_path: {self.data_path} \\n'\n",
    "              f'- n_users: {self.n_users} \\n'\n",
    "              f'- n_items: {self.n_items} \\n'\n",
    "              f'- n_interactions: {self.coo_matrix.nnz} \\n'\n",
    "              f'- split_set: {self.split_set} \\n'\n",
    "              f'- n_neg: {self.n_neg} \\n'\n",
    "              f'- neg_strategy: {self.neg_strategy} \\n')\n",
    "\n",
    "    def load_data(self):\n",
    "        print('Loading data')\n",
    "\n",
    "        user_ids = pd.read_csv(os.path.join(self.data_path, 'user_ids.csv'))\n",
    "        item_ids = pd.read_csv(os.path.join(self.data_path, 'item_ids.csv'))\n",
    "\n",
    "        self.n_users = len(user_ids)\n",
    "        self.n_items = len(item_ids)\n",
    "\n",
    "        train_lhs = pd.read_csv(os.path.join(self.data_path, 'listening_history_train.csv'))\n",
    "\n",
    "        train_csr = sp.csr_matrix(\n",
    "            (np.ones(len(train_lhs), dtype=np.int16), (train_lhs.user_id, train_lhs.item_id)),\n",
    "            shape=(self.n_users, self.n_items))\n",
    "\n",
    "        # Computing the popularity distribution (see _neg_sample_popular)\n",
    "        item_popularity = np.array(train_csr.sum(axis=0)).flatten()\n",
    "        self.pop_distribution = item_popularity / item_popularity.sum()\n",
    "\n",
    "        if self.split_set == 'val':\n",
    "            val_lhs = pd.read_csv(os.path.join(self.data_path, 'listening_history_val.csv'))\n",
    "\n",
    "            val_csr = sp.csr_matrix(\n",
    "                (np.ones(len(val_lhs), dtype=np.int16), (val_lhs.user_id, val_lhs.item_id)),\n",
    "                shape=(self.n_users, self.n_items))\n",
    "\n",
    "            val_coo = sp.coo_matrix(val_csr)\n",
    "\n",
    "            self.coo_matrix = val_coo\n",
    "            self.csr_matrix = val_csr + train_csr\n",
    "\n",
    "        elif self.split_set == 'test':\n",
    "            test_lhs = pd.read_csv(os.path.join(self.data_path, 'listening_history_test.csv'))\n",
    "\n",
    "            test_csr = sp.csr_matrix(\n",
    "                (np.ones(len(test_lhs), dtype=np.int16), (test_lhs.user_id, test_lhs.item_id)),\n",
    "                shape=(self.n_users, self.n_items))\n",
    "\n",
    "            test_coo = sp.coo_matrix(test_csr)\n",
    "\n",
    "            self.coo_matrix = test_coo\n",
    "            self.csr_matrix = test_csr + train_csr\n",
    "\n",
    "        elif self.split_set == 'train':\n",
    "            train_coo = sp.coo_matrix(train_csr)\n",
    "\n",
    "            self.coo_matrix = train_coo\n",
    "            self.csr_matrix = train_csr\n",
    "\n",
    "    def _neg_sample_uniform(self, row_idx: int) -> np.array:\n",
    "        \"\"\"\n",
    "        For a specific user, it samples n_neg items u.a.r.\n",
    "        :param row_idx: user id (or row in the matrix)\n",
    "        :return: npy array containing the negatively sampled items.\n",
    "        \"\"\"\n",
    "\n",
    "        consumed_items = self.csr_matrix.indices[self.csr_matrix.indptr[row_idx]:self.csr_matrix.indptr[row_idx + 1]]\n",
    "\n",
    "        # Uniform distribution without items consumed by the user\n",
    "        p = np.ones(self.n_items)\n",
    "        p[consumed_items] = 0.  # Excluding consumed items\n",
    "        p = p / p.sum()\n",
    "\n",
    "        sampled = np.random.choice(np.arange(self.n_items), self.n_neg, replace=False, p=p)\n",
    "\n",
    "        return sampled\n",
    "\n",
    "    def _neg_sample_popular(self, row_idx: int) -> np.array:\n",
    "        \"\"\"\n",
    "        For a specific user, it samples n_neg items considering the frequency of appearance of items in the dataset, i.e.\n",
    "        p(i being neg) ∝ (pop_i)^0.75.\n",
    "        :param row_idx: user id (or row in the matrix)\n",
    "        :return: npy array containing the negatively sampled items.\n",
    "        \"\"\"\n",
    "        consumed_items = self.csr_matrix.indices[self.csr_matrix.indptr[row_idx]:self.csr_matrix.indptr[row_idx + 1]]\n",
    "\n",
    "        p = self.pop_distribution.copy()\n",
    "        p[consumed_items] = 0.  # Excluding consumed items\n",
    "        p = np.power(p, .75)  # Squashing factor alpha = .75\n",
    "        p = p / p.sum()\n",
    "\n",
    "        sampled = np.random.choice(np.arange(self.n_items), self.n_neg, replace=False, p=p)\n",
    "        return sampled\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.coo_matrix.nnz\n",
    "\n",
    "    def __getitem__(self, index) -> T_co:\n",
    "        \"\"\"\n",
    "        Loads the (user,item) pair associated to the index and performs the negative sampling.\n",
    "        :param index: (user,item) index pair (as defined by the COO.data vector)\n",
    "        :return: (user_idx,item_idxs,labels) where\n",
    "            user_idx: is the index of the user\n",
    "            item_idxs: is a npy array containing the items indexes. The positive item is in the 1st position followed\n",
    "                        by the negative items indexes. Shape is (1 + n_neg,)\n",
    "            labels: npy array containing the labels. First position is 1, the others are 0. Shape is (1 + n_neg,).\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        user_idx = self.coo_matrix.row[index].astype('int64')\n",
    "        item_idx_pos = self.coo_matrix.col[index]\n",
    "\n",
    "        # Select the correct negative sampling strategy\n",
    "        if self.neg_strategy == 'uniform':\n",
    "            neg_samples = self._neg_sample_uniform(user_idx)\n",
    "        elif self.neg_strategy == 'popular':\n",
    "            neg_samples = self._neg_sample_popular(user_idx)\n",
    "        else:\n",
    "            raise ValueError(f'Negative Sampling Strategy <{self.neg_strategy}> not implemented ... Yet')\n",
    "\n",
    "        item_idxs = np.concatenate(([item_idx_pos], neg_samples)).astype('int64')\n",
    "\n",
    "        labels = np.zeros(1 + self.n_neg, dtype='float32')\n",
    "        labels[0] = 1.\n",
    "\n",
    "        return user_idx, item_idxs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b6bd828",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Built ProtoRecDataset module \n",
      "- data_path: ProtoMF\\data\\full_train \n",
      "- n_users: 6028 \n",
      "- n_items: 3123 \n",
      "- n_interactions: 559852 \n",
      "- split_set: train \n",
      "- n_neg: 10 \n",
      "- neg_strategy: uniform \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " array([ 222, 2623, 1293, 2567, 1742,   29,  677, 2661, 1222, 2242, 1227],\n",
       "       dtype=int64),\n",
       " array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = Path(r\".\\ProtoMF\\data\\full_train\")\n",
    "dataset = 'ml'\n",
    "tst= ProtoRecDataset(data_path, 'train', 10, 'uniform')\n",
    "tst.__getitem__(0)\n",
    "tst2 = data.DataLoader(tst)\n",
    "tst2\n",
    "\n",
    "tst.__getitem__(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e93f4018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_protorecdataset_dataloader(data_path: str, split_set: str, n_neg: int, neg_strategy='uniform',\n",
    "                                   **loader_params) -> data.DataLoader:\n",
    "    \"\"\"\n",
    "    Returns the dataloader for a ProtoRecDataset\n",
    "    :param data_path, ... ,neg_strategy: check ProtoRecDataset class for info about these parameters\n",
    "    :param loader_params: parameters for the Dataloader\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    protorec_dataset = ProtoRecDataset(data_path, split_set, n_neg, neg_strategy)\n",
    "    return data.DataLoader(protorec_dataset, **loader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56b296c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(conf, is_train: bool = True):\n",
    "    if is_train:\n",
    "        train_loader = get_protorecdataset_dataloader(\n",
    "        data_path= conf.data_path,\n",
    "        split_set='train',\n",
    "        n_neg=conf.neg_train,\n",
    "        neg_strategy=conf.train_neg_strategy,\n",
    "        batch_size=conf.train_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=conf.NUM_WORKERS,\n",
    "        prefetch_factor=5\n",
    "    )\n",
    "\n",
    "        val_loader = get_protorecdataset_dataloader(\n",
    "            data_path=conf.data_path,\n",
    "            split_set='val',\n",
    "            n_neg=conf.neg_val,\n",
    "            neg_strategy=conf.eval_neg_strategy,\n",
    "            batch_size=conf.val_batch_size,\n",
    "            num_workers=conf.NUM_WORKERS\n",
    "        )\n",
    "        \n",
    "        test_loader = get_protorecdataset_dataloader(\n",
    "            data_path=conf.data_path,\n",
    "            split_set='test',\n",
    "            n_neg=conf.neg_val,\n",
    "            neg_strategy=conf.eval_neg_strategy,\n",
    "            batch_size=conf.val_batch_size,\n",
    "            num_workers=conf.NUM_WORKERS\n",
    "        )\n",
    "\n",
    "        return {'train_loader': train_loader, 'val_loader': val_loader, 'test_loader': test_loader}\n",
    "    else:\n",
    "\n",
    "        test_loader = get_protorecdataset_dataloader(\n",
    "            data_path=conf.data_path,\n",
    "            split_set='test',\n",
    "            n_neg=conf.neg_val,\n",
    "            neg_strategy=conf.eval_neg_strategy,\n",
    "            batch_size=conf.val_batch_size,\n",
    "            num_workers=conf.NUM_WORKERS\n",
    "        )\n",
    "\n",
    "        return {'test_loader': test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723ce52c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c4bfd87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data_loaders_dict = load_data(user_proto_chose_original_hyper_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e6f023c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Built ProtoRecDataset module \n",
      "- data_path: ProtoMF\\data\\full_train \n",
      "- n_users: 6028 \n",
      "- n_items: 3123 \n",
      "- n_interactions: 559852 \n",
      "- split_set: train \n",
      "- n_neg: 99 \n",
      "- neg_strategy: uniform \n",
      "\n",
      "Loading data\n",
      "Built ProtoRecDataset module \n",
      "- data_path: ProtoMF\\data\\full_train \n",
      "- n_users: 6028 \n",
      "- n_items: 3123 \n",
      "- n_interactions: 13952 \n",
      "- split_set: val \n",
      "- n_neg: 99 \n",
      "- neg_strategy: uniform \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader = get_protorecdataset_dataloader(protomf_path / \"data/full_train\", 'train', 99, batch_size = 64)\n",
    "val_loader = get_protorecdataset_dataloader(protomf_path / \"data/full_train\", 'val', 99, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f902bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from omegaconf import OmegaConf\n",
    "\n",
    "# OmegaConf.create(proto_double_tie_chose_original_hyper_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b3c4c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_param = {\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'n_epochs': 10,\n",
    "    'eval_neg_strategy': 'uniform',\n",
    "    'val_batch_size': 64,\n",
    "    'train_batch_size': 64,\n",
    "    'data_path': protomf_path / \"data/ml\",\n",
    "    'NUM_WORKERS': 1,\n",
    "    'rec_sys_param': {'use_bias': 0},\n",
    "}\n",
    "\n",
    "base_hyper_params = {\n",
    "    **base_param,\n",
    "    'neg_train': 99,\n",
    "    'neg_val': 99,\n",
    "    'train_neg_strategy': 'uniform',#tune.choice(['popular', 'uniform']),\n",
    "    'loss_func_name': 'sampled_softmax', # tune.choice(['bce', 'bpr', 'sampled_softmax']),\n",
    "    'batch_size': 64,\n",
    "    'optim_param': {\n",
    "        'optim': 'adagrad',\n",
    "        'wd': 1e-4,\n",
    "        'lr': 1e-3\n",
    "    },\n",
    "}\n",
    "\n",
    "proto_double_tie_optimal_params = {\n",
    "    'loss_func_aggr': 'mean',\n",
    "    'ft_ext_param': {\n",
    "        \"ft_type\": \"prototypes_double_tie\",\n",
    "        'embedding_dim': 300, #tune.randint(10, 100),\n",
    "        'item_ft_ext_param': {\n",
    "            \"ft_type\": \"prototypes_double_tie\",\n",
    "            'sim_proto_weight': 1e-3,#tune.loguniform(1e-3, 10),\n",
    "            'sim_batch_weight': 1e-3,#tune.loguniform(1e-3, 10),\n",
    "            'use_weight_matrix': False,\n",
    "            'n_prototypes': 80, #tune.randint(10, 100),\n",
    "            'cosine_type': 'shifted',\n",
    "            'reg_proto_type': 'max',\n",
    "            'reg_batch_type': 'max'\n",
    "        },\n",
    "        'user_ft_ext_param': {\n",
    "            \"ft_type\": \"prototypes_double_tie\",\n",
    "            'sim_proto_weight': 1e-3,#tune.loguniform(1e-3, 10),\n",
    "            'sim_batch_weight': 1e-3, #tune.loguniform(1e-3, 10),\n",
    "            'use_weight_matrix': False,\n",
    "            'n_prototypes': 80, #tune.randint(10, 100),\n",
    "            'cosine_type': 'shifted',\n",
    "            'reg_proto_type': 'max',\n",
    "            'reg_batch_type': 'max'\n",
    "        },\n",
    "    },\n",
    "    \"checkpoint_dir\": 'experiments/full_train',\n",
    "    **base_hyper_params,\n",
    "}\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "# proto_double_tie_chose_original_hyper_params = argparse.Namespace(**proto_double_tie_chose_original_hyper_params)\n",
    "# proto_double_tie_chose_original_hyper_params = OmegaConf.create(proto_double_tie_chose_original_hyper_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab7f1df3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Building FeatureExtractor model ---\n",
      "Built Embedding model \n",
      "- n_objects: 6028 \n",
      "- embedding_dim: 300 \n",
      "- max_norm: None\n",
      "- only_positive: False\n",
      "Built PrototypeEmbedding model \n",
      "- n_prototypes: 80 \n",
      "- use_weight_matrix: False \n",
      "- sim_proto_weight: 0.001 \n",
      "- sim_batch_weight: 0.001 \n",
      "- reg_proto_type: max \n",
      "- reg_batch_type: max \n",
      "- cosine_type: shifted \n",
      "\n",
      "--- Finished building FeatureExtractor model ---\n",
      "\n",
      "--- Building FeatureExtractor model ---\n",
      "Built Embedding model \n",
      "- n_objects: 6028 \n",
      "- embedding_dim: 300 \n",
      "- max_norm: None\n",
      "- only_positive: False\n",
      "Built Embeddingw model \n",
      "- out_dimension: 80 \n",
      "- use_bias: False \n",
      "\n",
      "--- Finished building FeatureExtractor model ---\n",
      "\n",
      "--- Building FeatureExtractor model ---\n",
      "Built Embedding model \n",
      "- n_objects: 3123 \n",
      "- embedding_dim: 300 \n",
      "- max_norm: None\n",
      "- only_positive: False\n",
      "Built PrototypeEmbedding model \n",
      "- n_prototypes: 80 \n",
      "- use_weight_matrix: False \n",
      "- sim_proto_weight: 0.001 \n",
      "- sim_batch_weight: 0.001 \n",
      "- reg_proto_type: max \n",
      "- reg_batch_type: max \n",
      "- cosine_type: shifted \n",
      "\n",
      "--- Finished building FeatureExtractor model ---\n",
      "\n",
      "--- Building FeatureExtractor model ---\n",
      "Built Embedding model \n",
      "- n_objects: 3123 \n",
      "- embedding_dim: 300 \n",
      "- max_norm: None\n",
      "- only_positive: False\n",
      "Built Embeddingw model \n",
      "- out_dimension: 80 \n",
      "- use_bias: False \n",
      "\n",
      "--- Finished building FeatureExtractor model ---\n",
      "\n",
      "Built ConcatenateFeatureExtractors model \n",
      "- model_1: PrototypeEmbedding \n",
      "- model_2: EmbeddingW \n",
      "- invert: False \n",
      "\n",
      "Built ConcatenateFeatureExtractors model \n",
      "- model_1: PrototypeEmbedding \n",
      "- model_2: EmbeddingW \n",
      "- invert: True \n",
      "\n",
      "Built RecSys module \n",
      "- n_users: 6028 \n",
      "- n_items: 3123 \n",
      "- user_feature_extractor: ConcatenateFeatureExtractors \n",
      "- item_feature_extractor: ConcatenateFeatureExtractors \n",
      "- loss_func_name: sampled_softmax \n",
      "- use_bias: False \n",
      "\n",
      "Built Optimizer  \n",
      "- name: adagrad \n",
      "- lr: 0.001 \n",
      "- wd: 0.0001 \n",
      "\n",
      "Built Trainer module \n",
      "- n_epochs: 10 \n",
      "- loss_func_name: sampled_softmax \n",
      "- loss_func_aggr: mean \n",
      "- device: cuda \n",
      "- optimizing_metric: hit_ratio@10 \n",
      " - checkpoint_dir: experiments\\full_train\\12252023.222240 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "proto_double_tie_optimal_params = argparse.Namespace(**proto_double_tie_optimal_params)\n",
    "\n",
    "trainer = Trainer(train_loader, val_loader, proto_double_tie_optimal_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdc72c5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Building FeatureExtractor model ---\n",
      "Built Embedding model \n",
      "- n_objects: 6028 \n",
      "- embedding_dim: 300 \n",
      "- max_norm: None\n",
      "- only_positive: False\n",
      "Built PrototypeEmbedding model \n",
      "- n_prototypes: 80 \n",
      "- use_weight_matrix: False \n",
      "- sim_proto_weight: 0.001 \n",
      "- sim_batch_weight: 0.001 \n",
      "- reg_proto_type: max \n",
      "- reg_batch_type: max \n",
      "- cosine_type: shifted \n",
      "\n",
      "--- Finished building FeatureExtractor model ---\n",
      "\n",
      "--- Building FeatureExtractor model ---\n",
      "Built Embedding model \n",
      "- n_objects: 6028 \n",
      "- embedding_dim: 300 \n",
      "- max_norm: None\n",
      "- only_positive: False\n",
      "Built Embeddingw model \n",
      "- out_dimension: 80 \n",
      "- use_bias: False \n",
      "\n",
      "--- Finished building FeatureExtractor model ---\n",
      "\n",
      "--- Building FeatureExtractor model ---\n",
      "Built Embedding model \n",
      "- n_objects: 3123 \n",
      "- embedding_dim: 300 \n",
      "- max_norm: None\n",
      "- only_positive: False\n",
      "Built PrototypeEmbedding model \n",
      "- n_prototypes: 80 \n",
      "- use_weight_matrix: False \n",
      "- sim_proto_weight: 0.001 \n",
      "- sim_batch_weight: 0.001 \n",
      "- reg_proto_type: max \n",
      "- reg_batch_type: max \n",
      "- cosine_type: shifted \n",
      "\n",
      "--- Finished building FeatureExtractor model ---\n",
      "\n",
      "--- Building FeatureExtractor model ---\n",
      "Built Embedding model \n",
      "- n_objects: 3123 \n",
      "- embedding_dim: 300 \n",
      "- max_norm: None\n",
      "- only_positive: False\n",
      "Built Embeddingw model \n",
      "- out_dimension: 80 \n",
      "- use_bias: False \n",
      "\n",
      "--- Finished building FeatureExtractor model ---\n",
      "\n",
      "Built ConcatenateFeatureExtractors model \n",
      "- model_1: PrototypeEmbedding \n",
      "- model_2: EmbeddingW \n",
      "- invert: False \n",
      "\n",
      "Built ConcatenateFeatureExtractors model \n",
      "- model_1: PrototypeEmbedding \n",
      "- model_2: EmbeddingW \n",
      "- invert: True \n",
      "\n",
      "Built RecSys module \n",
      "- n_users: 6028 \n",
      "- n_items: 3123 \n",
      "- user_feature_extractor: ConcatenateFeatureExtractors \n",
      "- item_feature_extractor: ConcatenateFeatureExtractors \n",
      "- loss_func_name: sampled_softmax \n",
      "- use_bias: False \n",
      "\n",
      "Built Optimizer  \n",
      "- name: adagrad \n",
      "- lr: 0.001 \n",
      "- wd: 0.0001 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Adagrad (\n",
       "Parameter Group 0\n",
       "    differentiable: False\n",
       "    eps: 1e-10\n",
       "    foreach: None\n",
       "    initial_accumulator_value: 0\n",
       "    lr: 0.001\n",
       "    lr_decay: 0\n",
       "    maximize: False\n",
       "    weight_decay: 0.0001\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer._build_model()\n",
    "trainer._build_optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c94980b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation started\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d35c893b8e418ea185970eaaca9b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexey\\venvs\\recsys_project\\Lib\\site-packages\\ray\\train\\_internal\\session.py:638: UserWarning: `report` is meant to only be called inside a function that is executed by a Tuner or Trainer. Returning `None`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init - Avg Val Value 0.144 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Alexey\\Documents\\github\\hse_courses\\2nd_year\\term1\\recsys\\project\\rec_sys_folder\\trainer.py:124\u001b[0m, in \u001b[0;36mTrainer.run\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m    121\u001b[0m epoch_train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m u_idxs, i_idxs, labels \u001b[38;5;129;01min\u001b[39;00m track(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader)):\n\u001b[1;32m--> 124\u001b[0m     u_idxs \u001b[38;5;241m=\u001b[39m \u001b[43mu_idxs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m     i_idxs \u001b[38;5;241m=\u001b[39m i_idxs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    126\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "608d4f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_up = torch.load(checkpoint_dir + '/best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1debe6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_feats = np.array(model_up['item_feature_extractor.embedding_layer.weight'].to('cpu'))\n",
    "user_protos = np.array(model_up['user_feature_extractor.prototypes'].to('cpu'))\n",
    "user_embeds = np.array(model_up['user_feature_extractor.embedding_ext.embedding_layer.weight'].to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "705913a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_mat_users = np.array(((user_embeds.T) * 1 / np.linalg.norm(user_embeds, 2, axis=1)))\n",
    "normed_mat_protos = np.array(((user_protos.T) * (1 / np.linalg.norm(user_protos, 2, axis=1))))\n",
    "user_to_protos = (1 + np.dot(normed_mat_users.T, normed_mat_protos))\n",
    "scores = user_to_protos.dot(items_feats.T)\n",
    "top_20 = scores.argsort()[:, ::-1][:,:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6cb3bc63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 526,  222,  926, ...,   98,   48,  927],\n",
       "       [ 222,  926, 2257, ..., 2137, 2179,  930],\n",
       "       [ 926,  222, 2257, ...,   98,  944, 1254],\n",
       "       ...,\n",
       "       [ 926, 2257,  928, ..., 2137,  940, 2375],\n",
       "       [2257,  526,  222, ...,  511,    0,  944],\n",
       "       [ 526,  928,  222, ..., 2137,  864,  944]], dtype=int64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2cd29443",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid = pd.read_excel(r'C:\\Users\\aleke\\Downloads\\KION_DATASET\\ProtoMF\\data\\ml\\valid_ml_our_split.xlsx')\n",
    "test = pd.read_excel(r'C:\\Users\\aleke\\Downloads\\KION_DATASET\\ProtoMF\\data\\ml\\test_ml_our_split.xlsx')\n",
    "train= pd.read_excel(r'C:\\Users\\aleke\\Downloads\\KION_DATASET\\ProtoMF\\data\\ml\\train_ml_our_split.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "53f9cf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hr_score(top_n_items, real_likes):\n",
    "    mask = (top_n_items[...,None] == real_likes[:,None]).any(2)\n",
    "    return mask.any(axis=1).mean()\n",
    "\n",
    "def mrr_score(top_n_items, real_likes):\n",
    "    idx = np.arange(1, top_n_items.shape[1] + 1)[None, :]\n",
    "    mask = (top_n_items[...,None] == real_likes[:,None]).any(2)\n",
    "    return (mask / idx).max(axis=1).mean()\n",
    "\n",
    "def coverage_score(top_n_items, total_item_count):\n",
    "    return len(np.unique(top_n_items)) * 1.0 / total_item_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1e9ff40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_likes = test.groupby('userid')['itemid'].apply(len).max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cf4b7c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4154676258992806"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_likes = test.groupby('userid')['itemid'].apply(lambda x: list(np.pad(x, (0, max_likes - len(x)), 'constant', constant_values=-1)))\n",
    "\n",
    "test_users = test_likes.index\n",
    "test_likes = np.asarray(list(test_likes))\n",
    "hr_score(top_20[test_users], np.array(test_likes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
